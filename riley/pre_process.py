# -*- coding: utf-8 -*-
"""pre_process.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F4IpEvVCVm0oUIZ-FreqVMDdmUKRebSo
"""

import os
import pickle
import zipfile
import numpy as np
import scipy.signal
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split as tts
import torch
from torch.utils.data import DataLoader, TensorDataset as TData

# Unzip the data archive.
zip_file = "LHNT_EEG.zip"
if os.path.exists(zip_file):
    with zipfile.ZipFile(zip_file, "r") as z:
        z.extractall()
    print(f"Extracted {zip_file}.")

def getAllPickles(directory="LHNT EEG"):
    # List all subfolders and collect paths to all .pkl files.
    folders = [drctry for drctry in os.listdir(directory) if os.path.isdir(os.path.join(directory, drctry))]
    files = []
    for folder in folders:
        folder_files = os.listdir(os.path.join(directory, folder))
        for file in folder_files:
            if ".pkl" in file:
                files.append(os.path.join(directory, folder, file))
    return files

def npFromPickle(pickle_files):
    # Load each pickle file and assign a label:
    # 1 if filename contains "right", else 0.
    np_data = []
    labels = []  # 0 is left, 1 is right
    for file in pickle_files:
        with open(file, "rb") as f:
            data1 = pickle.load(f)
            np_data.append(data1[0])
        if 'right' in file.split(os.sep)[-1]:
            labels.append(1)
        else:
            labels.append(0)
    return np_data, labels

# Load data.
np_data, labels = npFromPickle(getAllPickles())
print("Total samples:", len(np_data), "Total labels:", len(labels))

def bandpass_filter(signal, crit_freq=[1, 40], sampling_freq=125, plot=False, channel=0):
    # Applies a Butterworth bandpass filter.
    order = 4
    b, a = scipy.signal.butter(order, crit_freq, btype='bandpass', fs=sampling_freq)
    processed_signal = scipy.signal.filtfilt(b, a, signal, axis=1)
    if plot:
        plt.figure()
        plt.xlabel('Time')
        plt.ylabel(f'Normalized amplitude of channel {channel}')
        plt.title(f'{crit_freq[0]}-{crit_freq[1]}Hz bandpass filter')
        signal_min = np.full(signal.shape, np.min(signal, axis=1, keepdims=True))
        signal_max = np.full(signal.shape, np.max(signal, axis=1, keepdims=True))
        normed_signal = (signal - signal_min) / (signal_max - signal_min)
        filtered_min = np.full(processed_signal.shape, np.min(processed_signal, axis=1, keepdims=True))
        filtered_max = np.full(processed_signal.shape, np.max(processed_signal, axis=1, keepdims=True))
        normed_filt = (processed_signal - filtered_min) / (filtered_max - filtered_min)
        plt.plot(np.arange(normed_signal[channel].size), normed_signal[channel], label='Input')
        plt.plot(np.arange(normed_filt[channel].size), normed_filt[channel], label='Transformed')
        plt.legend()
        plt.show()
    return processed_signal

def segmentation(signal, sampling_freq=125, window_size=1, window_shift=0.016):
    # Segments the signal into overlapping windows.
    w_size = int(sampling_freq * window_size)
    w_shift = int(sampling_freq * window_shift)
    segments = []
    i = 0
    while i + w_size <= signal.shape[1]:
        segments.append(signal[:, i: i + w_size])
        i += w_shift
    return segments

def channel_rearrangment(sig, channel_order):
    # Rearranges channels according to the provided order.
    # The channel_order is given in 1-indexed format.
    channel_order = [ch - 1 for ch in channel_order]
    reindexed = np.zeros_like(sig)
    for i, ind in enumerate(channel_order):
        reindexed[i] = sig[ind]
    return reindexed

def create_dataloaders(window_size=1.5, window_shift=0.0175, batch_size=64):
    # Define desired channel order.
    ordered_channels = [1, 9, 11, 3, 2, 12, 10, 4, 13, 5, 15, 7, 14, 16, 6, 8]

    # Split into train, validation, and test.
    train_x, test_x, train_y, test_y = tts(np_data, labels, test_size=0.25)
    val_x, test_x = test_x[:len(test_x)//2], test_x[len(test_x)//2:]
    val_y, test_y = test_y[:len(test_y)//2], test_y[len(test_y)//2:]

    train_eeg, train_labels = [], []
    valid_eeg, valid_labels = [], []
    test_eeg, test_labels   = [], []

    # Process train, validation, and test data.
    for sig, label in zip(train_x, train_y):
        if sig.shape[1] == 0:
            continue
        reindexed_signal = channel_rearrangment(sig, ordered_channels)
        filtered_sig = bandpass_filter(reindexed_signal, [5, 40], 125)
        normed_sig = (filtered_sig - np.mean(filtered_sig, axis=1, keepdims=True)) / np.std(filtered_sig, axis=1, keepdims=True)
        if np.isnan(normed_sig).any():
            continue
        signals = segmentation(normed_sig, 125, window_size, window_shift)
        train_eeg.extend(signals)
        train_labels.extend([label] * len(signals))

    for sig, label in zip(val_x, val_y):
        if sig.shape[1] == 0:
            continue
        reindexed_signal = channel_rearrangment(sig, ordered_channels)
        filtered_sig = bandpass_filter(reindexed_signal, [5, 40], 125)
        normed_sig = (filtered_sig - np.mean(filtered_sig, axis=1, keepdims=True)) / np.std(filtered_sig, axis=1, keepdims=True)
        if np.isnan(normed_sig).any():
            continue
        signals = segmentation(normed_sig, 125, window_size, window_shift)
        valid_eeg.extend(signals)
        valid_labels.extend([label] * len(signals))

    for sig, label in zip(test_x, test_y):
        if sig.shape[1] == 0:
            continue
        reindexed_signal = channel_rearrangment(sig, ordered_channels)
        filtered_sig = bandpass_filter(reindexed_signal, [5, 40], 125)
        normed_sig = (filtered_sig - np.mean(filtered_sig, axis=1, keepdims=True)) / np.std(filtered_sig, axis=1, keepdims=True)
        if np.isnan(normed_sig).any():
            continue
        signals = segmentation(normed_sig, 125, window_size, window_shift)
        test_eeg.extend(signals)
        test_labels.extend([label] * len(signals))

    # Optionally remove columns from each segment.
    columns_to_remove = [1, 2, 7, 8]
    train_eeg = [np.delete(arr, columns_to_remove, axis=1) for arr in train_eeg]
    valid_eeg = [np.delete(arr, columns_to_remove, axis=1) for arr in valid_eeg]
    test_eeg  = [np.delete(arr, columns_to_remove, axis=1) for arr in test_eeg]

    # Convert to PyTorch Tensors.
    def to_tensor(eeg_list):
        tensor = torch.zeros((len(eeg_list), eeg_list[0].shape[0], eeg_list[0].shape[1]))
        for i in range(len(eeg_list)):
            tensor[i] = torch.from_numpy(eeg_list[i].copy())
        return tensor

    train_eeg_tensor = to_tensor(train_eeg)
    valid_eeg_tensor = to_tensor(valid_eeg)
    test_eeg_tensor  = to_tensor(test_eeg)

    # Create one-hot encoded labels (2 classes: left and right).
    num_classes = 2
    def one_hot(labels_list):
        label_tensor = torch.zeros(len(labels_list), num_classes)
        for i, val in enumerate(labels_list):
            label_tensor[i][val] = 1
        return label_tensor

    train_label_tensor = one_hot(train_labels)
    valid_label_tensor = one_hot(valid_labels)
    test_label_tensor  = one_hot(test_labels)

    # Create TensorDatasets.
    train_ds = TData(train_eeg_tensor, train_label_tensor)
    valid_ds = TData(valid_eeg_tensor, valid_label_tensor)
    test_ds  = TData(test_eeg_tensor, test_label_tensor)

    # Create DataLoaders.
    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)
    valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, drop_last=True)
    test_dl  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, drop_last=True)

    print("Number of batches - Train:", len(train_dl), "Valid:", len(valid_dl), "Test:", len(test_dl))

    # Also return a full train loader without drop_last (for KMeans init).
    full_train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)

    return train_dl, valid_dl, test_dl, full_train_loader

if __name__ == "__main__":
    # When running this file, create the dataloaders.
    train_dl, valid_dl, test_dl, full_train_loader = create_dataloaders()